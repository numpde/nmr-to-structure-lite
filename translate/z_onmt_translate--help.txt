Some interesting options for onmt_translate include:

    Beam Search Tuning:
        -beam_size: Adjust the beam size.
        -length_penalty and -alpha: Control length bias.
        -coverage_penalty and -beta: Adjust coverage for better output.

    Unknown Token Handling:
        -replace_unk: Replace UNK tokens using source attention.
        -ban_unk_token: Prevent generation of UNK tokens.

    Output Diversity:
        -random_sampling_topk, -random_sampling_topp, -random_sampling_temp: For diverse, non-greedy decoding.
        -n_best: Output the top N translation candidates.
        -dump_beam: Dump detailed beam search logs.

    Debugging & Verbosity:
        -attn_debug and -align_debug: Provide insight into attention and alignment.
        -verbose: Print detailed scores and token-level predictions.

    Batch & Efficiency Options:
        --batch_size and --batch_type: Adjust how input is batched for faster decoding.


Full list of options (from onmt_translate --help):

usage: onmt_translate [-h] [-config CONFIG] [-save_config SAVE_CONFIG] --model
                      MODEL [MODEL ...] [--precision {,fp32,fp16,int8}]
                      [--fp32] [--int8] [--avg_raw_probs]
                      [--self_attn_type SELF_ATTN_TYPE]
                      [--data_type DATA_TYPE] --src SRC [--tgt TGT]
                      [--tgt_file_prefix] [--output OUTPUT] [--report_align]
                      [--gold_align] [--report_time] [--profile]
                      [-n_src_feats N_SRC_FEATS]
                      [-src_feats_defaults SRC_FEATS_DEFAULTS]
                      [--beam_size BEAM_SIZE] [--ratio RATIO]
                      [--random_sampling_topk RANDOM_SAMPLING_TOPK]
                      [--random_sampling_topp RANDOM_SAMPLING_TOPP]
                      [--random_sampling_temp RANDOM_SAMPLING_TEMP]
                      [--seed SEED] [--length_penalty {none,wu,avg}]
                      [--alpha ALPHA] [--coverage_penalty {none,wu,summary}]
                      [--beta BETA] [--stepwise_penalty]
                      [--min_length MIN_LENGTH] [--max_length MAX_LENGTH]
                      [--max_length_ratio MAX_LENGTH_RATIO]
                      [--block_ngram_repeat BLOCK_NGRAM_REPEAT]
                      [--ignore_when_blocking IGNORE_WHEN_BLOCKING [IGNORE_WHEN_BLOCKING ...]]
                      [--replace_unk] [--ban_unk_token]
                      [--phrase_table PHRASE_TABLE] [--log_file LOG_FILE]
                      [--log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}]
                      [--verbose] [--attn_debug] [--align_debug]
                      [--dump_beam DUMP_BEAM] [--n_best N_BEST] [--with_score]
                      [--gpu_ranks [GPU_RANKS ...]] [--world_size WORLD_SIZE]
                      [--parallel_mode {tensor_parallel,data_parallel}]
                      [--gpu_backend GPU_BACKEND]
                      [--gpu_verbose_level GPU_VERBOSE_LEVEL]
                      [--master_ip MASTER_IP] [--master_port MASTER_PORT]
                      [--timeout TIMEOUT] [--batch_size BATCH_SIZE]
                      [--batch_type {sents,tokens}] [--gpu GPU]
                      [-transforms {clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} [{clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} ...]]
                      [--src_eq_tgt] [--same_char] [--same_word]
                      [--scripts_ok [SCRIPTS_OK ...]]
                      [--scripts_nok [SCRIPTS_NOK ...]]
                      [--src_tgt_ratio SRC_TGT_RATIO]
                      [--avg_tok_min AVG_TOK_MIN] [--avg_tok_max AVG_TOK_MAX]
                      [--langid [LANGID ...]]
                      [--tags_dictionary_path TAGS_DICTIONARY_PATH]
                      [--tags_corpus_ratio TAGS_CORPUS_RATIO]
                      [--max_tags MAX_TAGS] [--paired_stag PAIRED_STAG]
                      [--paired_etag PAIRED_ETAG]
                      [--isolated_tag ISOLATED_TAG]
                      [--src_delimiter SRC_DELIMITER]
                      [-switchout_temperature SWITCHOUT_TEMPERATURE]
                      [-tokendrop_temperature TOKENDROP_TEMPERATURE]
                      [-tokenmask_temperature TOKENMASK_TEMPERATURE]
                      [--permute_sent_ratio PERMUTE_SENT_RATIO]
                      [--rotate_ratio ROTATE_RATIO]
                      [--insert_ratio INSERT_RATIO]
                      [--random_ratio RANDOM_RATIO] [--mask_ratio MASK_RATIO]
                      [--mask_length {subword,word,span-poisson}]
                      [--poisson_lambda POISSON_LAMBDA]
                      [--replace_length {-1,0,1}]
                      [-src_subword_model SRC_SUBWORD_MODEL]
                      [-tgt_subword_model TGT_SUBWORD_MODEL]
                      [-src_subword_nbest SRC_SUBWORD_NBEST]
                      [-tgt_subword_nbest TGT_SUBWORD_NBEST]
                      [-src_subword_alpha SRC_SUBWORD_ALPHA]
                      [-tgt_subword_alpha TGT_SUBWORD_ALPHA]
                      [-src_subword_vocab SRC_SUBWORD_VOCAB]
                      [-tgt_subword_vocab TGT_SUBWORD_VOCAB]
                      [-src_vocab_threshold SRC_VOCAB_THRESHOLD]
                      [-tgt_vocab_threshold TGT_VOCAB_THRESHOLD]
                      [-src_subword_type {none,sentencepiece,bpe}]
                      [-tgt_subword_type {none,sentencepiece,bpe}]
                      [-src_onmttok_kwargs SRC_ONMTTOK_KWARGS]
                      [-tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS] [--gpt2_pretok]
                      [--response_patterns RESPONSE_PATTERNS [RESPONSE_PATTERNS ...]]
                      [--src_seq_length SRC_SEQ_LENGTH]
                      [--tgt_seq_length TGT_SEQ_LENGTH]
                      [--src_prefix SRC_PREFIX] [--tgt_prefix TGT_PREFIX]
                      [--src_suffix SRC_SUFFIX] [--tgt_suffix TGT_SUFFIX]
                      [--reversible_tokenization {joiner,spacer}]
                      [--upper_corpus_ratio UPPER_CORPUS_RATIO]
                      [--termbase_path TERMBASE_PATH]
                      [--src_spacy_language_model SRC_SPACY_LANGUAGE_MODEL]
                      [--tgt_spacy_language_model TGT_SPACY_LANGUAGE_MODEL]
                      [--term_corpus_ratio TERM_CORPUS_RATIO]
                      [--term_example_ratio TERM_EXAMPLE_RATIO]
                      [--src_term_stoken SRC_TERM_STOKEN]
                      [--tgt_term_stoken TGT_TERM_STOKEN]
                      [--tgt_term_etoken TGT_TERM_ETOKEN]
                      [--term_source_delimiter TERM_SOURCE_DELIMITER]
                      [--doc_length DOC_LENGTH] [--max_context MAX_CONTEXT]
                      [--tm_path TM_PATH]
                      [--fuzzy_corpus_ratio FUZZY_CORPUS_RATIO]
                      [--fuzzy_threshold FUZZY_THRESHOLD]
                      [--tm_delimiter TM_DELIMITER]
                      [--fuzzy_token FUZZY_TOKEN]
                      [--fuzzymatch_min_length FUZZYMATCH_MIN_LENGTH]
                      [--fuzzymatch_max_length FUZZYMATCH_MAX_LENGTH]
                      [--src_lang SRC_LANG] [--tgt_lang TGT_LANG]
                      [--penn PENN] [--norm_quote_commas NORM_QUOTE_COMMAS]
                      [--norm_numbers NORM_NUMBERS]
                      [--pre_replace_unicode_punct PRE_REPLACE_UNICODE_PUNCT]
                      [--post_remove_control_chars POST_REMOVE_CONTROL_CHARS]
                      [--quant_layers QUANT_LAYERS [QUANT_LAYERS ...]]
                      [--quant_type {,bnb_8bit,bnb_FP4,bnb_NF4,awq_gemm,awq_gemv}]
                      [--w_bit {4}] [--group_size {128}]

translate.py

optional arguments:
  -h, --help            show this help message and exit

Configuration:
  -config CONFIG, --config CONFIG
                        Path of the main YAML config file. (default: None)
  -save_config SAVE_CONFIG, --save_config SAVE_CONFIG
                        Path where to save the config. (default: None)

Model:
  --model MODEL [MODEL ...], -model MODEL [MODEL ...]
                        Path to model .pt file(s). Multiple models can be
                        specified, for ensemble decoding. (default: [])
  --precision {,fp32,fp16,int8}, -precision {,fp32,fp16,int8}
                        Precision to run inference.default is model.dtypefp32
                        to force slow FP16 model on GTX1080int8 enables
                        pytorch native 8-bit quantization(cpu only) (default:
                        )
  --fp32, -fp32         Deprecated use 'precision' instead (default: None)
  --int8, -int8         Deprecated use 'precision' instead (default: None)
  --avg_raw_probs, -avg_raw_probs
                        If this is set, during ensembling scores from
                        different models will be combined by averaging their
                        raw probabilities and then taking the log. Otherwise,
                        the log probabilities will be averaged directly.
                        Necessary for models whose output layers can assign
                        zero probability. (default: False)
  --self_attn_type SELF_ATTN_TYPE, -self_attn_type SELF_ATTN_TYPE
                        Self attention type in Transformer decoder layer --
                        currently "scaled-dot", "scaled-dot-flash" or
                        "average" (default: scaled-dot-flash)

Data:
  --data_type DATA_TYPE, -data_type DATA_TYPE
                        Type of the source input. Options: [text]. (default:
                        text)
  --src SRC, -src SRC   Source sequence to decode (one line per sequence)
                        (default: None)
  --tgt TGT, -tgt TGT   True target sequence (optional) (default: None)
  --tgt_file_prefix, -tgt_file_prefix
                        Generate predictions using provided `-tgt` as prefix.
                        (default: False)
  --output OUTPUT, -output OUTPUT
                        Path to output the predictions (each line will be the
                        decoded sequence (default: pred.txt)
  --report_align, -report_align
                        Report alignment for each translation. (default:
                        False)
  --gold_align, -gold_align
                        Report alignment between source and gold target.Useful
                        to test the performance of learnt alignments.
                        (default: False)
  --report_time, -report_time
                        Report some translation time metrics (default: False)
  --profile, -profile   Report pytorch profiling stats (default: False)

Features:
  -n_src_feats N_SRC_FEATS, --n_src_feats N_SRC_FEATS
                        Number of source feats. (default: 0)
  -src_feats_defaults SRC_FEATS_DEFAULTS, --src_feats_defaults SRC_FEATS_DEFAULTS
                        Default features to apply in source in case there are
                        not annotated (default: None)

Beam Search:
  --beam_size BEAM_SIZE, -beam_size BEAM_SIZE
                        Beam size (default: 5)
  --ratio RATIO, -ratio RATIO
                        Ratio based beam stop condition (default: -0.0)

Random Sampling:
  --random_sampling_topk RANDOM_SAMPLING_TOPK, -random_sampling_topk RANDOM_SAMPLING_TOPK
                        Set this to -1 to do random sampling from full
                        distribution. Set this to value k>1 to do random
                        sampling restricted to the k most likely next tokens.
                        Set this to 1 to use argmax. (default: 0)
  --random_sampling_topp RANDOM_SAMPLING_TOPP, -random_sampling_topp RANDOM_SAMPLING_TOPP
                        Probability for top-p/nucleus sampling. Restrict
                        tokens to the most likely until the cumulated
                        probability is over p. In range [0, 1].
                        https://arxiv.org/abs/1904.09751 (default: 0.0)
  --random_sampling_temp RANDOM_SAMPLING_TEMP, -random_sampling_temp RANDOM_SAMPLING_TEMP
                        If doing random sampling, divide the logits by this
                        before computing softmax during decoding. (default:
                        1.0)
  --beam_size BEAM_SIZE, -beam_size BEAM_SIZE
                        Beam size (default: 5)

Reproducibility:
  --seed SEED, -seed SEED
                        Set random seed used for better reproducibility
                        between experiments. (default: -1)

Penalties:
  .. Note:: Coverage Penalty is not available in sampling.

  --length_penalty {none,wu,avg}, -length_penalty {none,wu,avg}
                        Length Penalty to use. (default: avg)
  --alpha ALPHA, -alpha ALPHA
                        Length penalty parameter(higher = longer generation)
                        (default: 1.0)
  --coverage_penalty {none,wu,summary}, -coverage_penalty {none,wu,summary}
                        Coverage Penalty to use. Only available in beam
                        search. (default: none)
  --beta BETA, -beta BETA
                        Coverage penalty parameter (default: -0.0)
  --stepwise_penalty, -stepwise_penalty
                        Apply coverage penalty at every decoding step. Helpful
                        for summary penalty. (default: False)

Decoding tricks:
  .. Tip:: Following options can be used to limit the decoding length or
  content.

  --min_length MIN_LENGTH, -min_length MIN_LENGTH
                        Minimum prediction length (default: 0)
  --max_length MAX_LENGTH, -max_length MAX_LENGTH
                        Maximum prediction length. (default: 250)
  --max_length_ratio MAX_LENGTH_RATIO, -max_length_ratio MAX_LENGTH_RATIO
                        Maximum prediction length ratio.for European languages
                        1.25 is large enoughfor target Asian characters need
                        to increase to 2-3for special languages (burmese,
                        amharic) to 10 (default: 1.25)
  --block_ngram_repeat BLOCK_NGRAM_REPEAT, -block_ngram_repeat BLOCK_NGRAM_REPEAT
                        Block repetition of ngrams during decoding. (default:
                        0)
  --ignore_when_blocking IGNORE_WHEN_BLOCKING [IGNORE_WHEN_BLOCKING ...], -ignore_when_blocking IGNORE_WHEN_BLOCKING [IGNORE_WHEN_BLOCKING ...]
                        Ignore these strings when blocking repeats. You want
                        to block sentence delimiters. (default: [])
  --replace_unk, -replace_unk
                        Replace the generated UNK tokens with the source token
                        that had highest attention weight. If phrase_table is
                        provided, it will look up the identified source token
                        and give the corresponding target token. If it is not
                        provided (or the identified source token does not
                        exist in the table), then it will copy the source
                        token. (default: False)
  --ban_unk_token, -ban_unk_token
                        Prevent unk token generation by setting unk proba to 0
                        (default: False)
  --phrase_table PHRASE_TABLE, -phrase_table PHRASE_TABLE
                        If phrase_table is provided (with replace_unk), it
                        will look up the identified source token and give the
                        corresponding target token. If it is not provided (or
                        the identified source token does not exist in the
                        table), then it will copy the source token. (default:
                        )

Logging:
  --log_file LOG_FILE, -log_file LOG_FILE
                        Output logs to a file under this path. (default: )
  --log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}, -log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}
  --verbose, -verbose   Print scores and predictions for each sentence
                        (default: False)
  --attn_debug, -attn_debug
                        Print best attn for each word (default: False)
  --align_debug, -align_debug
                        Print best align for each word (default: False)
  --dump_beam DUMP_BEAM, -dump_beam DUMP_BEAM
                        File to dump beam information to. (default: )
  --n_best N_BEST, -n_best N_BEST
                        If verbose is set, will output the n_best decoded
                        sentences (default: 1)
  --with_score, -with_score
                        add a tab separated score to the translation (default:
                        False)

Distributed:
  --gpu_ranks [GPU_RANKS ...], -gpu_ranks [GPU_RANKS ...]
                        list of ranks of each process. (default: [])
  --world_size WORLD_SIZE, -world_size WORLD_SIZE
                        total number of distributed processes. (default: 1)
  --parallel_mode {tensor_parallel,data_parallel}, -parallel_mode {tensor_parallel,data_parallel}
                        Distributed mode. (default: data_parallel)
  --gpu_backend GPU_BACKEND, -gpu_backend GPU_BACKEND
                        Type of torch distributed backend (default: nccl)
  --gpu_verbose_level GPU_VERBOSE_LEVEL, -gpu_verbose_level GPU_VERBOSE_LEVEL
                        Gives more info on each process per GPU. (default: 0)
  --master_ip MASTER_IP, -master_ip MASTER_IP
                        IP of master for torch.distributed training. (default:
                        localhost)
  --master_port MASTER_PORT, -master_port MASTER_PORT
                        Port of master for torch.distributed training.
                        (default: 10000)
  --timeout TIMEOUT, -timeout TIMEOUT
                        Timeout for one GOU to wait for the others. (default:
                        60)

Efficiency:
  --batch_size BATCH_SIZE, -batch_size BATCH_SIZE
                        Batch size (default: 30)
  --batch_type {sents,tokens}, -batch_type {sents,tokens}
                        Batch grouping for batch_size. Standard is sents.
                        Tokens will do dynamic batching (default: sents)
  --gpu GPU, -gpu GPU   Device to run on (default: -1)
  -transforms {clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} [{clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} ...], --transforms {clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} [{clean,inlinetags,switchout,tokendrop,tokenmask,bart,sentencepiece,bpe,onmt_tokenize,insert_mask_before_placeholder,filtertoolong,prefix,suffix,inferfeats,uppercase,terminology,docify,fuzzymatch,normalize} ...]
                        Default transform pipeline to apply to data. (default:
                        [])

Transform/Clean:
  --src_eq_tgt, -src_eq_tgt
                        Remove ex src==tgt (default: False)
  --same_char, -same_char
                        Remove ex with same char more than 4 times (default:
                        False)
  --same_word, -same_word
                        Remove ex with same word more than 3 times (default:
                        False)
  --scripts_ok [SCRIPTS_OK ...], -scripts_ok [SCRIPTS_OK ...]
                        list of unicodata scripts accepted (default: ['Latin',
                        'Common'])
  --scripts_nok [SCRIPTS_NOK ...], -scripts_nok [SCRIPTS_NOK ...]
                        list of unicodata scripts not accepted (default: [])
  --src_tgt_ratio SRC_TGT_RATIO, -src_tgt_ratio SRC_TGT_RATIO
                        ratio between src and tgt (default: 2)
  --avg_tok_min AVG_TOK_MIN, -avg_tok_min AVG_TOK_MIN
                        average length of tokens min (default: 3)
  --avg_tok_max AVG_TOK_MAX, -avg_tok_max AVG_TOK_MAX
                        average length of tokens max (default: 20)
  --langid [LANGID ...], -langid [LANGID ...]
                        list of languages accepted (default: [])

Transform/InlineTags:
  --tags_dictionary_path TAGS_DICTIONARY_PATH, -tags_dictionary_path TAGS_DICTIONARY_PATH
                        Path to a flat term dictionary. (default: None)
  --tags_corpus_ratio TAGS_CORPUS_RATIO, -tags_corpus_ratio TAGS_CORPUS_RATIO
                        Ratio of corpus to augment with tags. (default: 0.1)
  --max_tags MAX_TAGS, -max_tags MAX_TAGS
                        Maximum number of tags that can be added to a single
                        sentence. (default: 12)
  --paired_stag PAIRED_STAG, -paired_stag PAIRED_STAG
                        The format of an opening paired inline tag. Must
                        include the character #. (default: ｟ph_#_beg｠)
  --paired_etag PAIRED_ETAG, -paired_etag PAIRED_ETAG
                        The format of a closing paired inline tag. Must
                        include the character #. (default: ｟ph_#_end｠)
  --isolated_tag ISOLATED_TAG, -isolated_tag ISOLATED_TAG
                        The format of an isolated inline tag. Must include the
                        character #. (default: ｟ph_#_std｠)
  --src_delimiter SRC_DELIMITER, -src_delimiter SRC_DELIMITER
                        Any special token used for augmented src sentences.
                        The default is the fuzzy token used in the FuzzyMatch
                        transform. (default: ｟fuzzy｠)

Transform/SwitchOut:
  -switchout_temperature SWITCHOUT_TEMPERATURE, --switchout_temperature SWITCHOUT_TEMPERATURE
                        Sampling temperature for SwitchOut. :math:`\tau^{-1}`
                        in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller
                        value makes data more diverse. (default: 1.0)

Transform/Token_Drop:
  -tokendrop_temperature TOKENDROP_TEMPERATURE, --tokendrop_temperature TOKENDROP_TEMPERATURE
                        Sampling temperature for token deletion. (default:
                        1.0)

Transform/Token_Mask:
  -tokenmask_temperature TOKENMASK_TEMPERATURE, --tokenmask_temperature TOKENMASK_TEMPERATURE
                        Sampling temperature for token masking. (default: 1.0)

Transform/BART:
  --permute_sent_ratio PERMUTE_SENT_RATIO, -permute_sent_ratio PERMUTE_SENT_RATIO
                        Permute this proportion of sentences (boundaries
                        defined by ['.', '?', '!']) in all inputs. (default:
                        0.0)
  --rotate_ratio ROTATE_RATIO, -rotate_ratio ROTATE_RATIO
                        Rotate this proportion of inputs. (default: 0.0)
  --insert_ratio INSERT_RATIO, -insert_ratio INSERT_RATIO
                        Insert this percentage of additional random tokens.
                        (default: 0.0)
  --random_ratio RANDOM_RATIO, -random_ratio RANDOM_RATIO
                        Instead of using <mask>, use random token this often.
                        (default: 0.0)
  --mask_ratio MASK_RATIO, -mask_ratio MASK_RATIO
                        Fraction of words/subwords that will be masked.
                        (default: 0.0)
  --mask_length {subword,word,span-poisson}, -mask_length {subword,word,span-poisson}
                        Length of masking window to apply. (default: subword)
  --poisson_lambda POISSON_LAMBDA, -poisson_lambda POISSON_LAMBDA
                        Lambda for Poisson distribution to sample span length
                        if `-mask_length` set to span-poisson. (default: 3.0)
  --replace_length {-1,0,1}, -replace_length {-1,0,1}
                        When masking N tokens, replace with 0, 1, or N tokens.
                        (use -1 for N) (default: -1)

Transform/Subword/Common:
  .. Attention:: Common options shared by all subword transforms. Including
  options for indicate subword model path, `Subword Regularization
  <https://arxiv.org/abs/1804.10959>`_/`BPE-Dropout
  <https://arxiv.org/abs/1910.13267>`_, and `Vocabulary Restriction
  <https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-
  pair-encoding-in-nmt>`__.

Transform/Subword/Common:
  .. Attention:: Common options shared by all subword transforms. Including
  options for indicate subword model path, `Subword Regularization
  <https://arxiv.org/abs/1804.10959>`_/`BPE-Dropout
  <https://arxiv.org/abs/1910.13267>`_, and `Vocabulary Restriction
  <https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-
  pair-encoding-in-nmt>`__.

Transform/Subword/Common:
  .. Attention:: Common options shared by all subword transforms. Including
  options for indicate subword model path, `Subword Regularization
  <https://arxiv.org/abs/1804.10959>`_/`BPE-Dropout
  <https://arxiv.org/abs/1910.13267>`_, and `Vocabulary Restriction
  <https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-
  pair-encoding-in-nmt>`__.

  -src_subword_model SRC_SUBWORD_MODEL, --src_subword_model SRC_SUBWORD_MODEL
                        Path of subword model for src (or shared). (default:
                        None)
  -tgt_subword_model TGT_SUBWORD_MODEL, --tgt_subword_model TGT_SUBWORD_MODEL
                        Path of subword model for tgt. (default: None)
  -src_subword_nbest SRC_SUBWORD_NBEST, --src_subword_nbest SRC_SUBWORD_NBEST
                        Number of candidates in subword regularization. Valid
                        for unigram sampling, invalid for BPE-dropout. (source
                        side) (default: 1)
  -tgt_subword_nbest TGT_SUBWORD_NBEST, --tgt_subword_nbest TGT_SUBWORD_NBEST
                        Number of candidates in subword regularization. Valid
                        for unigram sampling, invalid for BPE-dropout. (target
                        side) (default: 1)
  -src_subword_alpha SRC_SUBWORD_ALPHA, --src_subword_alpha SRC_SUBWORD_ALPHA
                        Smoothing parameter for sentencepiece unigram
                        sampling, and dropout probability for BPE-dropout.
                        (source side) (default: 0)
  -tgt_subword_alpha TGT_SUBWORD_ALPHA, --tgt_subword_alpha TGT_SUBWORD_ALPHA
                        Smoothing parameter for sentencepiece unigram
                        sampling, and dropout probability for BPE-dropout.
                        (target side) (default: 0)
  -src_subword_vocab SRC_SUBWORD_VOCAB, --src_subword_vocab SRC_SUBWORD_VOCAB
                        Path to the vocabulary file for src subword. Format:
                        <word> <count> per line. (default: )
  -tgt_subword_vocab TGT_SUBWORD_VOCAB, --tgt_subword_vocab TGT_SUBWORD_VOCAB
                        Path to the vocabulary file for tgt subword. Format:
                        <word> <count> per line. (default: )
  -src_vocab_threshold SRC_VOCAB_THRESHOLD, --src_vocab_threshold SRC_VOCAB_THRESHOLD
                        Only produce src subword in src_subword_vocab with
                        frequency >= src_vocab_threshold. (default: 0)
  -tgt_vocab_threshold TGT_VOCAB_THRESHOLD, --tgt_vocab_threshold TGT_VOCAB_THRESHOLD
                        Only produce tgt subword in tgt_subword_vocab with
                        frequency >= tgt_vocab_threshold. (default: 0)

Transform/Subword/ONMTTOK:
  -src_subword_type {none,sentencepiece,bpe}, --src_subword_type {none,sentencepiece,bpe}
                        Type of subword model for src (or shared) in
                        pyonmttok. (default: none)
  -tgt_subword_type {none,sentencepiece,bpe}, --tgt_subword_type {none,sentencepiece,bpe}
                        Type of subword model for tgt in pyonmttok. (default:
                        none)
  -src_onmttok_kwargs SRC_ONMTTOK_KWARGS, --src_onmttok_kwargs SRC_ONMTTOK_KWARGS
                        Other pyonmttok options for src in dict string, except
                        subword related options listed earlier. (default:
                        {'mode': 'none'})
  -tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS, --tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS
                        Other pyonmttok options for tgt in dict string, except
                        subword related options listed earlier. (default:
                        {'mode': 'none'})
  --gpt2_pretok, -gpt2_pretok
                        Preprocess sentence with byte-level mapping (default:
                        False)

Transform/InsertMaskBeforePlaceholdersTransform:
  --response_patterns RESPONSE_PATTERNS [RESPONSE_PATTERNS ...], -response_patterns RESPONSE_PATTERNS [RESPONSE_PATTERNS ...]
                        Response patten to locate the end of the prompt
                        (default: ['Response : ｟newline｠'])

Transform/Filter:
  --src_seq_length SRC_SEQ_LENGTH, -src_seq_length SRC_SEQ_LENGTH
                        Maximum source sequence length. (default: 192)
  --tgt_seq_length TGT_SEQ_LENGTH, -tgt_seq_length TGT_SEQ_LENGTH
                        Maximum target sequence length. (default: 192)

Transform/Prefix:
  --src_prefix SRC_PREFIX, -src_prefix SRC_PREFIX
                        String to prepend to all source example. (default: )
  --tgt_prefix TGT_PREFIX, -tgt_prefix TGT_PREFIX
                        String to prepend to all target example. (default: )

Transform/Suffix:
  --src_suffix SRC_SUFFIX, -src_suffix SRC_SUFFIX
                        String to append to all source example. (default: )
  --tgt_suffix TGT_SUFFIX, -tgt_suffix TGT_SUFFIX
                        String to append to all target example. (default: )

Transform/InferFeats:
  --reversible_tokenization {joiner,spacer}, -reversible_tokenization {joiner,spacer}
                        Type of reversible tokenization applied on the
                        tokenizer. (default: joiner)

Transform/Uppercase:
  --upper_corpus_ratio UPPER_CORPUS_RATIO, -upper_corpus_ratio UPPER_CORPUS_RATIO
                        Corpus ratio to apply uppercasing. (default: 0.01)

Transform/Terminology:
  --termbase_path TERMBASE_PATH, -termbase_path TERMBASE_PATH
                        Path to a dictionary file with terms. (default: None)
  --src_spacy_language_model SRC_SPACY_LANGUAGE_MODEL, -src_spacy_language_model SRC_SPACY_LANGUAGE_MODEL
                        Name of the spacy language model for the source
                        corpus. (default: None)
  --tgt_spacy_language_model TGT_SPACY_LANGUAGE_MODEL, -tgt_spacy_language_model TGT_SPACY_LANGUAGE_MODEL
                        Name of the spacy language model for the target
                        corpus. (default: None)
  --term_corpus_ratio TERM_CORPUS_RATIO, -term_corpus_ratio TERM_CORPUS_RATIO
                        Ratio of corpus to augment with terms. (default: 0.3)
  --term_example_ratio TERM_EXAMPLE_RATIO, -term_example_ratio TERM_EXAMPLE_RATIO
                        Max terms allowed in an example. (default: 0.2)
  --src_term_stoken SRC_TERM_STOKEN, -src_term_stoken SRC_TERM_STOKEN
                        The source term start token. (default:
                        ｟src_term_start｠)
  --tgt_term_stoken TGT_TERM_STOKEN, -tgt_term_stoken TGT_TERM_STOKEN
                        The target term start token. (default:
                        ｟tgt_term_start｠)
  --tgt_term_etoken TGT_TERM_ETOKEN, -tgt_term_etoken TGT_TERM_ETOKEN
                        The target term end token. (default: ｟tgt_term_end｠)
  --term_source_delimiter TERM_SOURCE_DELIMITER, -term_source_delimiter TERM_SOURCE_DELIMITER
                        Any special token used for augmented source sentences.
                        The default is the fuzzy token used in the FuzzyMatch
                        transform. (default: ｟fuzzy｠)

Transform/Docify:
  --doc_length DOC_LENGTH, -doc_length DOC_LENGTH
                        Number of tokens per doc. (default: 200)
  --max_context MAX_CONTEXT, -max_context MAX_CONTEXT
                        Max context segments. (default: 1)

Transform/FuzzyMatching:
  --tm_path TM_PATH, -tm_path TM_PATH
                        Path to a flat text TM. (default: None)
  --fuzzy_corpus_ratio FUZZY_CORPUS_RATIO, -fuzzy_corpus_ratio FUZZY_CORPUS_RATIO
                        Ratio of corpus to augment with fuzzy matches.
                        (default: 0.1)
  --fuzzy_threshold FUZZY_THRESHOLD, -fuzzy_threshold FUZZY_THRESHOLD
                        The fuzzy matching threshold. (default: 70)
  --tm_delimiter TM_DELIMITER, -tm_delimiter TM_DELIMITER
                        The delimiter used in the flat text TM. (default: )
  --fuzzy_token FUZZY_TOKEN, -fuzzy_token FUZZY_TOKEN
                        The fuzzy token to be added with the matches.
                        (default: ｟fuzzy｠)
  --fuzzymatch_min_length FUZZYMATCH_MIN_LENGTH, -fuzzymatch_min_length FUZZYMATCH_MIN_LENGTH
                        Min length for TM entries and examples to match.
                        (default: 4)
  --fuzzymatch_max_length FUZZYMATCH_MAX_LENGTH, -fuzzymatch_max_length FUZZYMATCH_MAX_LENGTH
                        Max length for TM entries and examples to match.
                        (default: 70)

Transform/Normalize:
  --src_lang SRC_LANG, -src_lang SRC_LANG
                        Source language code (default: )
  --tgt_lang TGT_LANG, -tgt_lang TGT_LANG
                        Target language code (default: )
  --penn PENN, -penn PENN
                        Penn substitution (default: True)
  --norm_quote_commas NORM_QUOTE_COMMAS, -norm_quote_commas NORM_QUOTE_COMMAS
                        Normalize quotations and commas (default: True)
  --norm_numbers NORM_NUMBERS, -norm_numbers NORM_NUMBERS
                        Normalize numbers (default: True)
  --pre_replace_unicode_punct PRE_REPLACE_UNICODE_PUNCT, -pre_replace_unicode_punct PRE_REPLACE_UNICODE_PUNCT
                        Replace unicode punct (default: False)
  --post_remove_control_chars POST_REMOVE_CONTROL_CHARS, -post_remove_control_chars POST_REMOVE_CONTROL_CHARS
                        Remove control chars (default: False)

Quant options:
  --quant_layers QUANT_LAYERS [QUANT_LAYERS ...], -quant_layers QUANT_LAYERS [QUANT_LAYERS ...]
                        list of layers to be compressed in 4/8bit. (default:
                        [])
  --quant_type {,bnb_8bit,bnb_FP4,bnb_NF4,awq_gemm,awq_gemv}, -quant_type {,bnb_8bit,bnb_FP4,bnb_NF4,awq_gemm,awq_gemv}
                        Type of compression. (default: )
  --w_bit {4}, -w_bit {4}
                        W_bit quantization. (default: 4)
  --group_size {128}, -group_size {128}
                        group size quantization. (default: 128)

Args that start with '--' can also be set in a config file (specified via
-config). The config file uses YAML syntax and must represent a YAML 'mapping'
(for details, see http://learn.getgrav.org/advanced/yaml). In general,
command-line values override config file values which override defaults.

